{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "PD_proj2_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamemc/PD_02/blob/EMC/PD_03_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcgwnRfLgHaf"
      },
      "source": [
        "# Data Mining / Prospecção de Dados\n",
        "\n",
        "## Diogo F. Soares and Sara C. Madeira, 2020/21\n",
        "\n",
        "# Project 2 - Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjLy5zShgHat"
      },
      "source": [
        "## Logistics \n",
        "\n",
        "**_Read Carefully_**\n",
        "\n",
        "**Students should work in teams of 2 or 3 people**. \n",
        "\n",
        "Individual projects might be allowed (with valid justification), but will not have better grades for this reason. \n",
        "\n",
        "The quality of the project will dictate its grade, not the number of people working.\n",
        "\n",
        "**The project's solution should be uploaded in Moodle before the end of `April, 18th (23:59)`.** \n",
        "\n",
        "Students should **upload a `.zip` file** containing all the files necessary for project evaluation. \n",
        "Groups should be registered in [Moodle](https://moodle.ciencias.ulisboa.pt/mod/groupselect/view.php?id=139096) and the zip file should be identified as `PDnn.zip` where `nn` is the number of your group.\n",
        "\n",
        "**It is mandatory to produce a Jupyter notebook containing code and text/images/tables/etc describing the solution and the results. Projects not delivered in this format will not be graded. You can use `PD_202021_P2.ipynb`as template. In your `.zip` folder you should also include an HTML version of your notebook with all the outputs** (File > Download as > HTML).\n",
        "\n",
        "**Decisions should be justified and results should be critically discussed.** \n",
        "\n",
        "_Project solutions containing only code and outputs without discussions will achieve a maximum grade 10 out of 20._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPNMHG2UgHau"
      },
      "source": [
        "## Dataset and Tools\n",
        "\n",
        "In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and **[Scikit-learn](http://scikit-learn.org/stable/)**.\n",
        "\n",
        "The dataset to be analysed is **`medulloblastoma_genes.csv`**. It includes 76 samples of medulloblastoma (MB) with respective expression levels of 54.675 genes measured in children with ages between 3 and 16 years. Medulloblastoma is a malignant childhood brain tumour comprising four discrete subgroups. \n",
        "\n",
        "In this project you will consider the labels of the samples included in the `labels.csv` file where samples are labelled as MB-CL or Other. In this case, we have 51 samples of classic medulloblastoma (MB-CL) and 25 other types (namely: 6 desmoplastic nodular, 17 anaplastic and 2 medullomyoblastoma).\n",
        "\n",
        "In `medulloblastoma_genes.csv` each line represents a sample and each column represents a gene.\n",
        "\n",
        "\n",
        "**The goal is to cluster samples and (ideally) find \"MB-CL\" groups and \"Other MB\" groups.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ou1gN7mgHaw"
      },
      "source": [
        "## Team Identification\n",
        "\n",
        "**GROUP NNN**\n",
        "\n",
        "Students:\n",
        "\n",
        "* **Eduardo Carvalho - nº55881**\n",
        "* **Filipe Santos - nº55142**\n",
        "* **Ivo Oliveira - nº50301**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5cgX4OKgHax"
      },
      "source": [
        "## 1. Load and Preprocess Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jdu8-3GgHay"
      },
      "source": [
        "At the end of this step you should have:\n",
        "* a 76 rows × 54675 columns matrix, **X**, containing the values of the 54675 features for each of the 76 samples.\n",
        "* a vector, **y**, with the 76 type of medulloblastoma, which you can use later to evaluate clustering quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_6KhFQ9gHaz"
      },
      "source": [
        "# Importing relevant libraries\n",
        "from pandas import read_csv, DataFrame, Series\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics.cluster import contingency_matrix"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYVVMmahJD5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc80698-a0a6-483d-e2a5-1b4550030985"
      },
      "source": [
        "X=read_csv('/content/medulloblastoma_genes.csv', index_col=0)\n",
        "X.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76, 54675)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns5Xcwd_KD6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a350e7-e096-4f34-b301-d61ae52fc3c2"
      },
      "source": [
        "y=read_csv('/content/labels.csv', index_col=0)\n",
        "y.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYGI4lacqtQD"
      },
      "source": [
        "# X, y = df.iloc[:, 1:].values, df2.iloc[:, 1].values\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "#                      test_size=0.3, \n",
        "#                      random_state=0, \n",
        "#                      stratify=y)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r934_PYgHa0"
      },
      "source": [
        "## 2. Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9j7y-UugHa1"
      },
      "source": [
        "As you already noticed the number of features (genes) is extremely high when compared to the number of objects to cluster (samples). In this context, you should perform dimensionality reduction, that is, reduce the number of features, in two ways:\n",
        "\n",
        "* [**Removing features with low variance**](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
        "\n",
        "* [**Using Principal Component Analysis**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "\n",
        "At the end of this step you should have two new matrices with the same number of rows, each with a different number of columns (features): **X_variance** and **X_PCA**. \n",
        "\n",
        "**Don't change X you will need it!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIK_7eB0zKYc"
      },
      "source": [
        "### Low variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxwcEcV2zUj8"
      },
      "source": [
        "var_thresh = VarianceThreshold(threshold=0)\n",
        "var_X = var_thresh.fit_transform(X)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "xp05zg8T0MQF",
        "outputId": "cc2895a3-0319-4703-aa8e-64e7266b18a5"
      },
      "source": [
        "df_var=DataFrame(var_thresh.variances_, columns=['Variance'])\\\n",
        "                .sort_values(by='Variance', ascending=False)\n",
        "df_var.head(30).plot.bar()\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Variance');\n",
        "plt.legend('')\n",
        "plt.tight_layout()\n",
        "plt.gcf().set_size_inches(20, 8)\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-368c3ea1d92a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_thresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Variance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Variance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataFrame' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9myuzaQ8OBc"
      },
      "source": [
        "> By looking at the plot, we can identify the first 5 values as a good cutoff, as these features should a considerably higher variance than the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-waO5bWM8cRa"
      },
      "source": [
        "df_var.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c089JFj8_n3"
      },
      "source": [
        "> The threshold for the variance, to keep the 5 highest values, seems to be 6. We can now create a new VarianceThreshold object with this information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ICyC_8E96iD"
      },
      "source": [
        "new_vt = VarianceThreshold(threshold=6)\n",
        "new_vt.fit_transform(X)\n",
        "mask=new_vt.get_support()\n",
        "X_variance=X.loc[:, mask]\n",
        "X_variance.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKvnwzrRPUCD"
      },
      "source": [
        "> Variance is influenced by the scale of the values. We can try to standardize the values, by dividing them for each feature's mean, and see if the results are significantly different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4rLqaMpNZry"
      },
      "source": [
        "X_means = X / X.mean()\n",
        "var_thresh_2 = VarianceThreshold(threshold=0)\n",
        "var_X_2 = var_thresh_2.fit_transform(X_means)\n",
        "df_var_2=DataFrame(var_thresh_2.variances_, columns=['Variance'])\\\n",
        "                .sort_values(by='Variance', ascending=False)\n",
        "                \n",
        "df_var_2.head(30).plot.bar()\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Variance');\n",
        "plt.legend('')\n",
        "plt.tight_layout()\n",
        "plt.gcf().set_size_inches(20, 8)\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ogF8NMP-mq"
      },
      "source": [
        "> The results appear to be the same. The most obvious cutoff point is 2, with 5 being the next. So, we can continue with the previous selection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4bGTR1-zH0d"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsmFngC5nMH0"
      },
      "source": [
        "# Standardizing the features\n",
        "std_X = StandardScaler().fit_transform(X)\n",
        "std_X[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQacVQQGvrXd"
      },
      "source": [
        "pca = PCA()\n",
        "df_pca = DataFrame(pca.fit_transform(std_X))\n",
        "df_pca.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqNhK0h_v_N1"
      },
      "source": [
        "#DataFrame(pca.explained_variance_ratio_).head(30).plot.bar()\n",
        "#plt.legend('')\n",
        "#plt.tight_layout()\n",
        "#plt.gcf().set_size_inches(10, 4)\n",
        "#plt.xlabel('Principal Components')\n",
        "#plt.ylabel('Explained Variance');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoCEP6shxHoh"
      },
      "source": [
        "> Looking at the top 30 components in terms of explained variance, we can see some considerable drops at 3 points: from the 2nd to the 3rd, from the 3rd to the 4th and from the 6th to the 7th. Afterwards, the drop in explained variance is less noticeable.\n",
        ">\n",
        "> We will choose to keep 3 principal components as this number allows for easier comprehension of the dataset while still keeping a high amount of information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-GDoaw7oQoT"
      },
      "source": [
        "pca = PCA(n_components=3)\n",
        "pca.fit(std_X)\n",
        "X_PCA = pca.transform(std_X)\n",
        "X_PCA[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDqnnbEOgHa4"
      },
      "source": [
        "## 3. Clustering Samples using Partitional Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntQXlyNAgHa6"
      },
      "source": [
        "Use **`K`-means** to cluster the samples:\n",
        "\n",
        "* Cluster the original data (54.675 features): **X**.\n",
        "    * Use different values of `K`.\n",
        "    * For each value of `K` present the clustering by specifying how many samples MB-CL and Other are in each cluster.     \n",
        "    For instance, `{0: {'MB-CL': 51, 'Other': 0}, 1: {'MB-CL': 0, 'Other': 25}}` is the ideal clustering that we aimed at obtained with K-means when `K=2`, where the first cluster has 51 MB-CL samples and 0 Other samples and the second cluster has 0 MB-CL samples and 25 Other samples.\n",
        "    You can choose how to output this information.  **Tip**: You can explore the usage of contigency matrices.\n",
        "    * What is the best value of `K` ? Justify using the clustering results and the [Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
        "\n",
        "* Cluster the data obtained after removing features with low variance: **X_variance**.\n",
        "    * Study different values of `K` as above.\n",
        "\n",
        "* Cluster the data obtained after applying PCA: **X_PCA**.\n",
        "    * Study different values of `K` as above.\n",
        "\n",
        "* Compare the results obtained in the three datasets above for the best `K`. \n",
        "* Discuss the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmqQvAJ6R4YU"
      },
      "source": [
        "## K-means - X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zkCYtXGR-uc"
      },
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans = kmeans.fit(std_X) #using standardized data\n",
        "kmeans.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PWIGNQRgHa7"
      },
      "source": [
        "kmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuc22sOyYFZK"
      },
      "source": [
        "cm_X=contingency_matrix(y['class'], kmeans.labels_)\n",
        "sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "plt.show()\n",
        "#most_bought.fig.set_figheight(12.5)\n",
        "#most_bought.fig.set_figwidth(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiF8GhHfeEoS"
      },
      "source": [
        "> In this case we have 30 MB-CL samples in cluster 0 and 21 in cluster 1. We also have 7 \"Other\" samples in cluster 0 and 18 in cluster 1. The samples do not appear to be correctly separated according to the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNyfgIm4WEYy"
      },
      "source": [
        "### Determining ideal number of clusters by Elbow Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWUKiCdMVQOZ"
      },
      "source": [
        "cost =[]\n",
        "for i in range(1, 21):\n",
        "    KM = KMeans(n_clusters = i, max_iter = 500)\n",
        "    KM.fit(std_X)\n",
        "      \n",
        "    # calculates squared error for the clustered points\n",
        "    cost.append(KM.inertia_)   \n",
        "\n",
        "# plot the cost against K values\n",
        "plt.plot(range(1, 21), cost, color ='g', linewidth ='3')\n",
        "plt.xlabel(\"Value of K\")\n",
        "plt.ylabel(\"Squared Error (Cost)\")\n",
        "plt.tight_layout()\n",
        "plt.gcf().set_size_inches(10, 5)\n",
        "plt.xticks(rotation = 45)\n",
        "plt.ticklabel_format(style='plain')\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYgxs242WJSK"
      },
      "source": [
        "> Using the method of Elbow Plot does not seem to be very helpful, as there is no discernible cutoff point that we can use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7DgtCGkWSKu"
      },
      "source": [
        "### Determining ideal number of clusters by Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Vv8wVFWWe5"
      },
      "source": [
        "silhouette_coefficients = []\n",
        "for i in range(2, 11):\n",
        "  kmeans = KMeans(n_clusters=i)\n",
        "  kmeans.fit(std_X)\n",
        "  score = silhouette_score(std_X, kmeans.labels_)\n",
        "  silhouette_coefficients.append(score)\n",
        "\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "plt.plot(range(2, 11), silhouette_coefficients)\n",
        "plt.xticks(range(2, 11))\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Coefficient\")\n",
        "plt.tight_layout()\n",
        "plt.gcf().set_size_inches(20, 8)\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5cXc9ofXpI2"
      },
      "source": [
        "> The best number of clusters seems to be 2 as it has the highest value for the silhouette coefficient. In this case, we won't have to run the algorithm again, as we already used k=2 in the previous run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcPVCExmybG6"
      },
      "source": [
        "for i in range(2,11):\n",
        "  fig, ax = plt.subplots(figsize=(8,8))\n",
        "  plt.suptitle('n_cluster = ' + str(i))\n",
        "  kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "  kmeans = kmeans.fit(std_X)\n",
        "  cm_X=contingency_matrix(y['class'], kmeans.labels_)\n",
        "  sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hDcqFs1m-Wh"
      },
      "source": [
        "### Running k-means for different values of k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PntDb6z-fQT5"
      },
      "source": [
        "for i in range(2,11):\n",
        "  fig, ax = plt.subplots(figsize=(8,6))\n",
        "  plt.suptitle('n_cluster = ' + str(i))\n",
        "  kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "  kmeans = kmeans.fit(std_X)\n",
        "  cm_X=contingency_matrix(y['class'], kmeans.labels_)\n",
        "  sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "             yticklabels=['MB-CL','Other'])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcLndOdUoB5w"
      },
      "source": [
        "> Looking at the contingency matrices for k between 2 and 10, we can see that, for k=4 and k=5, there are clusters where the predominant label is 'MB-CL' (cluster 3 for k=4, cluster 2 for k=5).\n",
        ">\n",
        "> Between these two, k=5 seems to be the case where this distinction is more evident, as the remaining clusters together don't have near as many 'MB-CL' labels as cluster 2 (18 vs 33).\n",
        ">\n",
        "> There isn't a k value where a cluster is predominantly composed of the label 'Other'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4OYD6hxpJWd"
      },
      "source": [
        "> Based on the silhouette score and the contingency matrices, we conclude that, for the dataset with all features and standardized values, the ideal number of clusters is 2 - the silhouette score is the highest and there is an acceptable separation between labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r5O3w5xqHlL"
      },
      "source": [
        "## K-means - X_variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfPEFXTXqHlQ"
      },
      "source": [
        "# Standardizing the features\n",
        "std_X_var = StandardScaler().fit_transform(X_variance)\n",
        "\n",
        "# K-means\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans = kmeans.fit(std_X_var)\n",
        "kmeans.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDESdMKqHlY"
      },
      "source": [
        "kmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d99c0WgCqHlk"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "plt.suptitle('n_cluster = 4')\n",
        "cm_X_var=contingency_matrix(y['class'], kmeans.labels_)\n",
        "sn.heatmap(cm_X_var, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsscs0itqHlm"
      },
      "source": [
        "> In this case we have 15 MB-CL samples in cluster 0 and 36 in cluster 1. We also have 7 \"Other\" samples in cluster 0 and 18 in cluster 1. Again, the samples do not appear to be correctly separated according to the true labels as both clusters appear to have a ratio of 2:1 in terms of labels (with predominance for label 'MB-CL', in both cases)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URuxQbcnqHln"
      },
      "source": [
        "### Determining ideal number of clusters by Elbow Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVeNmQnTqHlp"
      },
      "source": [
        "cost =[]\n",
        "for i in range(1, 21):\n",
        "    KM = KMeans(n_clusters = i, max_iter = 500)\n",
        "    KM.fit(std_X_var)\n",
        "      \n",
        "    # calculates squared error for the clustered points\n",
        "    cost.append(KM.inertia_)     \n",
        "  \n",
        "# plot the cost against K values\n",
        "fig, ax = plt.subplots(figsize=(16,6))\n",
        "plt.plot(range(1, 21), cost, color ='g', linewidth ='3')\n",
        "plt.xlabel(\"Value of K\")\n",
        "plt.ylabel(\"Squared Error (Cost)\")\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQCAmOrJqHlr"
      },
      "source": [
        "> This time we can actually spot an elbow shape in the plot, with the ideal value for k being between 4 and 6 (where the plot starts to stabilize)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvqsrCbhqHlt"
      },
      "source": [
        "### Determining ideal number of clusters by Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf5yy6JUqHlu"
      },
      "source": [
        "silhouette_coefficients = []\n",
        "for i in range(2, 11):\n",
        "  kmeans = KMeans(n_clusters=i)\n",
        "  kmeans.fit(std_X_var)\n",
        "  score = silhouette_score(std_X_var, kmeans.labels_)\n",
        "  silhouette_coefficients.append(score)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,6))\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "plt.plot(range(2, 11), silhouette_coefficients)\n",
        "plt.xticks(range(2, 11))\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Coefficient\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7NXPRKFqHlx"
      },
      "source": [
        "> The highest values for the silhouette coefficient are for k=10, 9 and. K=6 also shows a high score and it also matches with the observations made in elbow plot, appearing to be the most likely candidate thus far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FowpNEuFqHly"
      },
      "source": [
        "### Running k-means for different values of k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHl_i3IgqHlz"
      },
      "source": [
        "for i in range(2,11):\n",
        "  fig, ax = plt.subplots(figsize=(8,6))\n",
        "  plt.suptitle('n_cluster = ' + str(i))\n",
        "  kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "  kmeans = kmeans.fit(std_X_var)\n",
        "  cm_X=contingency_matrix(y['class'], kmeans.labels_)\n",
        "  sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pn3Q6LGqHlz"
      },
      "source": [
        "> There isn't really a k value where the the separation between labels is clear but k=6, as we have seen before, has two clusters with mostly 'MB-CL' labels. K=5 also shows the same level of separation.\n",
        ">\n",
        "> Again, there is no cluster where the label 'Other' is clearly predominant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-WgxDxqHl0"
      },
      "source": [
        "> Based on the silhouette score and the contingency matrices, we conclude that, for the dataset with only the high variance features and standardized values, the ideal number of clusters is 6 - the silhouette score is very high, the elbow plot starts to stabilize from that point forward and there is an acceptable separation between labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6YkgROvDzC"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "plt.suptitle('n_cluster = ' + str(i))\n",
        "# K-means\n",
        "kmeans = KMeans(n_clusters=6, random_state=0)\n",
        "kmeans = kmeans.fit(std_X_var)\n",
        "# continency matrix\n",
        "cm_X_var=contingency_matrix(y['class'], kmeans.labels_)\n",
        "\n",
        "sn.heatmap(cm_X_var, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksmi75NwqEyd"
      },
      "source": [
        "## K-means - X_PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIBh1rOku_DX"
      },
      "source": [
        "# Standardizing the features\n",
        "std_X_PCA = StandardScaler().fit_transform(X_PCA)\n",
        "\n",
        "# K-means\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans = kmeans.fit(std_X_PCA)\n",
        "kmeans.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PniHqJtqEyj"
      },
      "source": [
        "kmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve5IDl5-qEyp"
      },
      "source": [
        "cm_X_PCA=contingency_matrix(y['class'], kmeans.labels_)\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sn.heatmap(cm_X_PCA, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTdGodAaqEys"
      },
      "source": [
        "> We have 13 MB-CL samples in cluster 0 and 38 in cluster 1. There is a higher level of separation here than on the cases we have seen before. We also have 11 \"Other\" samples in cluster 0 and 14 in cluster 1. The samples for 'Other' still do not appear to be correctly separated according to the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tI7P8kNqEyt"
      },
      "source": [
        "### Determining ideal number of clusters by Elbow Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXviOMIqEyu"
      },
      "source": [
        "cost =[]\n",
        "for i in range(1, 21):\n",
        "    KM = KMeans(n_clusters = i, max_iter = 500)\n",
        "    KM.fit(std_X_PCA)\n",
        "      \n",
        "    # calculates squared error for the clustered points\n",
        "    cost.append(KM.inertia_)     \n",
        "  \n",
        "# plot the cost against K values\n",
        "fig, ax = plt.subplots(figsize=(16,6))\n",
        "plt.plot(range(1, 21), cost, color ='g', linewidth ='3')\n",
        "plt.xlabel(\"Value of K\")\n",
        "plt.ylabel(\"Squared Error (Cost)\")\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7SWLtfVqEyw"
      },
      "source": [
        "> Like in the previous case, there is a noticeable elbow shape situated between k=4 and k=8. The ideal number of clusters should be somewhere between that interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o22MMvG7qEyz"
      },
      "source": [
        "### Determining ideal number of clusters by Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecWgcz4jqEy1"
      },
      "source": [
        "silhouette_coefficients = []\n",
        "for i in range(2, 11):\n",
        "  kmeans = KMeans(n_clusters=i)\n",
        "  kmeans.fit(std_X_PCA)\n",
        "  score = silhouette_score(std_X_PCA, kmeans.labels_)\n",
        "  silhouette_coefficients.append(score)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,6))\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "plt.plot(range(2, 11), silhouette_coefficients)\n",
        "plt.xticks(range(2, 11))\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Coefficient\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnJNIZ7UqEy7"
      },
      "source": [
        "> The silhouette coefficient is higher for k values between 8 and 10 (9 being the highest). However, there is a considerable increase in the score for k=4, a value that is inside the interval determined by the elbow plot.\n",
        ">\n",
        "> For these reasons, k=4 seems to be the primary candidate for the number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFhazLWjqEy8"
      },
      "source": [
        "### Running k-means for different values of k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G07XagcIqEy9"
      },
      "source": [
        "for i in range(2,11):\n",
        "  fig, ax = plt.subplots(figsize=(8,6))\n",
        "  plt.suptitle('n_cluster = ' + str(i))\n",
        "  kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "  kmeans = kmeans.fit(std_X_PCA)\n",
        "  cm_X=contingency_matrix(y['class'], kmeans.labels_)\n",
        "  sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h_uM8ctqEy-"
      },
      "source": [
        "> Looking at the contingency matrices for k between 2 and 10, we can see that, for k=4 there are 2 clusters where the predominant label is 'MB-CL' (cluster 1 and 2), whereas for for k=5 there are 3 (clusters 1, 2 and 3).\n",
        ">\n",
        "> This seems to corroborate the previous assertion that k=4 is the best value for k, for the dataset after PCA.\n",
        ">\n",
        "> Again, there isn't a k value for which a cluster is predominantly composed of the label 'Other'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8XA7c91qEy-"
      },
      "source": [
        "> Based on the silhouette score, contingency matrices and elbow plot, we conclude that, for the dataset after PCA and standardized values, the ideal number of clusters is 4 - its silhouette score displays a big increase, the elbow plot seems to stabilize around that region and there is an acceptable separation between labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSFe3dlmKPNj"
      },
      "source": [
        "# K-means\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans = kmeans.fit(std_X_PCA)\n",
        "# continency matrix\n",
        "cm_X_PCA=contingency_matrix(y['class'], kmeans.labels_)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sn.heatmap(cm_X_PCA, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiiCDdFyMD7Z"
      },
      "source": [
        "## Comparison of Results and Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81r5DExYMIWW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYCzZo4KgHa7"
      },
      "source": [
        "## 4. Clustering Samples using Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvW5iQOOgHa8"
      },
      "source": [
        "Use a **Hierarchical Clustering Algorithm (HCA)** to cluster the samples: \n",
        "\n",
        "* Cluster the data in **X_variance**.\n",
        "    * Use **different linkage metrics**.\n",
        "    * Use different values of `K`.\n",
        "    * For each linkage metric and value of `K` present the clustering by specifying how many MB-CL and Other samples are in each cluster as you did before. \n",
        "    * What is the best linkage metric and the best value of `K`? Justify using the clustering results and the [Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
        "\n",
        "* Cluster the data in **X_PCA**.\n",
        "    * Study different linkage metrics and different values of `K` as above.\n",
        "\n",
        "* Compare the results obtained in the two datasets above for the best linkage metric and the best `K`. \n",
        "* Discuss the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DYFFhaUo13"
      },
      "source": [
        "# Different linkage metrics for k=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifqFeSBsgHa8"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "#linkage{‘ward’, ‘complete’, ‘average’, ‘single’}\n",
        "#‘ward’ minimizes the variance of the clusters being merged.\n",
        "#‘average’ uses the average of the distances of each observation of the two sets.\n",
        "#‘complete’ or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.\n",
        "#‘single’ uses the minimum of the distances between all observations of the two sets.\n",
        "\n",
        "std_X_var = StandardScaler().fit_transform(X_variance)\n",
        "std_X_PCA = StandardScaler().fit_transform(X_PCA)\n",
        "\n",
        "#hca = AgglomerativeClustering(linkage =\"average\", n_clusters=2)\n",
        "# Grow the cluster hierarchy until K = 2 (dendogram)\n",
        "#hca = hca.fit(std_X_var)\n",
        "# Cluster memmberships\n",
        "#hca.labels_\n",
        "#cm_X_var1=contingency_matrix(y['class'], hca.labels_)\n",
        "#sn.heatmap(cm_X_var1, annot=True, cmap=\"rocket\", cbar=False, \n",
        "#           yticklabels=['MB-CL','Other'])\n",
        "#plt.show()\n",
        "\n",
        "linkage= [\"average\",\"ward\",\"complete\",\"single\"]\n",
        "for i in linkage:\n",
        "  fig, ax = plt.subplots(figsize=(8,6))\n",
        "  plt.suptitle('\\nlinkage_metric = ' + str(i))\n",
        "  hca = AgglomerativeClustering(linkage =i, n_clusters=6)\n",
        "  hca = hca.fit(std_X_var)\n",
        "  cm_X=contingency_matrix(y['class'], hca.labels_)\n",
        "  sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLlOI54OUs6p"
      },
      "source": [
        "# Silhoutte score for various linkages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wreL2ohUu75"
      },
      "source": [
        "linkage= [\"average\",\"ward\",\"complete\",\"single\"]\n",
        "for j in linkage:\n",
        "  fig, ax = plt.subplots(figsize=(16,6))\n",
        "  plt.suptitle('\\nlinkage_metric = ' + str(j))\n",
        "  silhouette_coefficients = []\n",
        "  for i in range(2, 11):\n",
        "    hca = AgglomerativeClustering(linkage =j, n_clusters=i)\n",
        "    hca = hca.fit(std_X_var)\n",
        "    score = silhouette_score(std_X_var, hca.labels_)\n",
        "    silhouette_coefficients.append(score)\n",
        "\n",
        "  plt.style.use(\"fivethirtyeight\")\n",
        "  plt.plot(range(2, 11), silhouette_coefficients)\n",
        "  plt.xticks(range(2, 11))\n",
        "  plt.xlabel(\"Number of Clusters\")\n",
        "  plt.ylabel(\"Silhouette Coefficient\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au9REg0QgHa9"
      },
      "source": [
        "## 5. Evaluating Clustering Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faLkp-0lU2i6"
      },
      "source": [
        "#best k for linkage \"ward\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owLGFds4U4Lh"
      },
      "source": [
        "for i in range(2,11):\n",
        "  fig, ax = plt.subplots(figsize=(8,6))\n",
        "  plt.suptitle('\\nlinkage_metric = ' + str(i))\n",
        "  hca = AgglomerativeClustering(linkage =\"ward\", n_clusters=i)\n",
        "  hca = hca.fit(std_X_var)\n",
        "  cm_X=contingency_matrix(y['class'], hca.labels_)\n",
        "  sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "           yticklabels=['MB-CL','Other'])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr7OEBL3gHa-"
      },
      "source": [
        "### 5.1. Without Using Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X3R6kgOgHa9"
      },
      "source": [
        "In this task you should compare the best results obtained using `K`-means and HCA \n",
        "1. **Without using ground truth**\n",
        "2. **Using ground truth (`Medulloblastoma Type`)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGRLCEvFgHa-"
      },
      "source": [
        "**Choose one adequate measure** from those available by Sciki-learn (https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) to evaluate the different clusterings. \n",
        "\n",
        "Discuss the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbAz69-FK-fx"
      },
      "source": [
        "Calinski Harabasz metric for evaluation without prior knowledge of Ground Truth was chosen due to it's ease of calculation\n",
        "\n",
        "\n",
        "Like most internal clustering criteria, Calinski-Harabasz is a heuristic device. The proper way to use it is to compare clustering solutions obtained on the same data, - solutions which differ either by the number of clusters or by the clustering method used.\n",
        "\n",
        "There is no \"acceptable\" cut-off value. You simply compare CH values by eye. The higher the value, the \"better\" is the solution. If on the line-plot of CH values there appears that one solution give a peak or at least an abrupt elbow, choose it. If, on the contrary, the line is smooth - horizontal or ascending or descending - then there is no reason to prefer one solution to others.\n",
        "\n",
        "CH criterion is based on ANOVA ideology. Hence, it implies that the clustered objects lie in Euclidean space of scale (not ordinal or binary or nominal) variables. If the data clustered were not objects X variables but a matrix of dissimilarities between objects then the dissimilarity measure should be (squared) euclidean distance (or, at worse, am other metric distance approaching euclidean distance by properties).\n",
        "\n",
        "CH criterion is most suitable in case when clusters are more or less spherical and compact in their middle (such as normally distributed, for instance)1\n",
        ". Other conditions being equal, CH tends to prefer cluster solutions with clusters consisting of roughly the same number of objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZaaokz1VAGV"
      },
      "source": [
        "The Calinski-Harabasz index also known as the Variance Ratio Criterion, is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters, the higher the score , the better the performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3fPzuI4MNqM"
      },
      "source": [
        "from sklearn.metrics.cluster import calinski_harabasz_score\n",
        "\n",
        "ch_scores = []\n",
        "for i in range(2, 11):\n",
        "  kmeans = KMeans(n_clusters=i)\n",
        "  kmeans.fit(std_X)\n",
        "  score = calinski_harabasz_score(std_X, kmeans.labels_)\n",
        "  ch_scores.append(score)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,6))\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "plt.plot(range(2, 11), ch_scores)#, label='Model length')\n",
        "plt.suptitle(\"Calinski Harabasz Score of K-means\")\n",
        "\n",
        "#legend = ax.legend(loc='upper right', fontsize='x-small')\n",
        "#legend.get_frame().set_facecolor('C0')\n",
        "\n",
        "plt.xticks(range(2, 11))\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Calinski-Harabasz Score\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyF11_NuaZvl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feq70qa8bPvx"
      },
      "source": [
        "ch2_scores1 = []\n",
        "for i in range(2, 11):\n",
        "  hca1 = AgglomerativeClustering(n_clusters=i)\n",
        "  hca1 = hca1.fit(std_X_var)\n",
        "  score = silhouette_score(std_X_var, hca1.labels_)\n",
        "  ch2_scores1.append(score)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,6))\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "plt.plot(range(2, 11), ch2_scores1)#, label='Model length')\n",
        "plt.suptitle(\"Calinski Harabasz Score of HCA\")\n",
        "\n",
        "#legend = ax.legend(loc='upper left', fontsize='x-small')\n",
        "#legend.get_frame().set_facecolor('C0')\n",
        "\n",
        "plt.xticks(range(2, 11))\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Calinski-Harabasz Score\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzWVDzk0gHa_"
      },
      "source": [
        "### 5.2. Using Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdDYzM3gHbA"
      },
      "source": [
        "**Choose one adequate measure** from those available by Sciki-learn (https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) to evaluate the different clusterings. \n",
        "\n",
        "Discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBA-AEiCgHbA"
      },
      "source": [
        "from sklearn.metrics.cluster import silhouette_score, silhouette_samples\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "def drawSillouette(X, labels, header=\"\" ):\n",
        "    y_lower = 10\n",
        "    clusters=list(set(labels))\n",
        "    n_clusters=len(clusters)\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    ax1 = plt.gca()\n",
        "    ax1.set_xlim([-0.5, 1])\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters) * 3+ y_lower])\n",
        "    sil_avg = silhouette_score(X, labels)\n",
        "    for i,c in enumerate(clusters):\n",
        "        silhouette_values = silhouette_samples(X, labels)\n",
        "        cs_values = silhouette_values[labels == c]\n",
        "        cs_values.sort()\n",
        "        size_ci = cs_values.shape[0]\n",
        "        y_upper = y_lower + size_ci\n",
        "        color = cm.nipy_spectral(i / n_clusters) \n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, cs_values, facecolor=color, edgecolor=\"k\", alpha=0.7)\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_ci, str(c))\n",
        "        y_lower = y_upper + 3  \n",
        "    ax1.set_title(\"Silhouette plot \" + header+ str())\n",
        "    ax1.set_xlabel(\"\\nSilhouette coefficient\\n\\n\\n\"+\"Average Silhouette Score: \"+str(sil_avg))\n",
        "    ax1.set_ylabel(\"Clusters\")\n",
        "    ax1.axvline(x=sil_avg, c=\"r\", linestyle=\":\")\n",
        "    ax1.set_yticks([]) \n",
        "    plt.show()\n",
        "\n",
        "drawSillouette(std_X, kmeans.labels_, \"of K-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuB9N5BmoQkv"
      },
      "source": [
        "drawSillouette(std_X, hca.labels_, \"of HCA\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bVEwknqVILN"
      },
      "source": [
        "The Silhouette Coefficient is defined for each sample and is composed of two scores(shown in below), and a higher Silhouette Coefficient score relates to a model with better defined clusters.\n",
        "\n",
        "The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n",
        "\n",
        "The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEPZbrLJgHbA"
      },
      "source": [
        "## 6. Clustering Samples using Density-based Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGKXwic5gHbC"
      },
      "source": [
        "Use DBSCAN (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) or OPTICS (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) to cluster the samples.\n",
        "\n",
        "Compare the results with those of K-means and HCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Ci7YIALh5o"
      },
      "source": [
        "First of all, DBSCAN produces noise. And it's unclear how to correctly compute these indexes when there is noise. If you treat every noise point as its own cluster, the points will give you, e.g., a bad Silhouette, even when this is exactly the desire behavior. If you pretend noise is a cluster, the result will even be worse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crE-SsUSs37p"
      },
      "source": [
        "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "\n",
        "optics_model = OPTICS(min_samples = 10, xi = 0.05, min_cluster_size = 0.05).fit(std_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aJYdVxu634U"
      },
      "source": [
        "print(std_X.max(), std_X.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQnQEwXyu2La"
      },
      "source": [
        "colors = list(map(lambda x: 'b' if x == 1 else 'r', kmeans.labels_)) #hca.labels_      kmeans.labels_\n",
        "plt.scatter(std_X[:,0], std_X[:,1], c=colors, marker=\"o\", picker=True)\n",
        "#plt.scatter(std_X[:,0], std_X[:,-8])\n",
        "#plt.scatter(std_X[:,0], std_X[:,9], color='r')\n",
        "plt.grid(True)\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTE4jlqurACU"
      },
      "source": [
        "colors = list(map(lambda x: 'b' if x == 1 else 'r', hca.labels_)) #hca.labels_      kmeans.labels_\n",
        "plt.scatter(std_X[:,0], std_X[:,1], c=colors, marker=\"o\", picker=True)\n",
        "#plt.scatter(std_X[:,0], std_X[:,-8])\n",
        "#plt.scatter(std_X[:,0], std_X[:,9], color='r')\n",
        "plt.grid(True)\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUUxPfKNgHbD"
      },
      "source": [
        "## 7. Choose a Different Clustering Algorithm to Group the Samples\n",
        "\n",
        "Choose **a clustering algorithm** besides `K`-means, HCA and DBSCAN/OPTICS to cluster the samples. \n",
        "\n",
        "**Groups of 3 People** must choose two different algorithms.\n",
        "\n",
        "Justify your choice and compare the results with those of `K`-means, HCA and DBSCAN/OPTICS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw11lO9TgHbD"
      },
      "source": [
        "from sklearn import cluster\n",
        "#spectral = cluster.SpectralClustering(n_clusters=2,\n",
        "#                                          eigen_solver='arpack',\n",
        "#                                          affinity=\"nearest_neighbors\")\n",
        "#spectral = spectral.fit(std_X_var)\n",
        "#spectral\n",
        "#\n",
        "#cm_X=contingencymatrix(y['class'], spectral.labels)\n",
        "#sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "#         yticklabels=['MB-CL','Other'])\n",
        "#plt.show()\n",
        "\n",
        "##affinity_propagation = cluster.AffinityPropagation(damping=.9,\n",
        "##                                                       preference=-50)\n",
        "##affinity_propagation = affinity_propagation.fit(std_X_var)\n",
        "##affinity_propagation\n",
        "##cm_X=contingency_matrix(y['class'], affinitypropagation.labels)\n",
        "##sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "##         yticklabels=['MB-CL','Other'])\n",
        "##plt.show()\n",
        "\n",
        "meanshift= cluster.MeanShift(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=1)\n",
        "meanshift= meanshift.fit(std_X_var)\n",
        "cm_X=contingency_matrix(y['class'], meanshift.labels_)\n",
        "sn.heatmap(cm_X, annot=True, cmap=\"rocket\", cbar=False, \n",
        "         yticklabels=['MB-CL','Other'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3eMghytgHbE"
      },
      "source": [
        "## 8. Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-J7U1AgHbE"
      },
      "source": [
        "Draw some conclusions about this project work. Can you highlight some insights about meduloblastoma types? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eFOGLiQgHbE"
      },
      "source": [
        "# Write code in cells like this ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQZbHT_tgHbF"
      },
      "source": [
        "Write text in cells like this..."
      ]
    }
  ]
}